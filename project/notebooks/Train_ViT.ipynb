{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT) Training with PyTorch\n",
    "\n",
    "In this notebook, we will train a Vision Transformer (ViT) model for real-time sign language detection using PyTorch. We will use Albumentations for data augmentation and PyTorch's modules for model building and training.\n",
    "\n",
    "The training will involve the following steps:\n",
    "1. Data augmentation and preprocessing\n",
    "2. Loading the dataset\n",
    "3. Defining and compiling the Vision Transformer model\n",
    "4. Training the model\n",
    "5. Saving the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.13). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from albumentations import Compose, Normalize, Resize, RandomCrop, HorizontalFlip, RandomBrightnessContrast\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "We will use Albumentations to perform various data augmentation techniques, including resizing, cropping, flipping, and normalization. These augmentations will help improve the generalization of the model. We also create a custom `AlbumentationsDataset` class to apply these transformations to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation pipeline\n",
    "def get_augmentations():\n",
    "    return Compose([\n",
    "        Resize(224, 224),  # Resize images to the input size of ViT\n",
    "        RandomCrop(224, 224),\n",
    "        HorizontalFlip(),\n",
    "        RandomBrightnessContrast(),\n",
    "        Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "class AlbumentationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = datasets.ImageFolder(image_folder)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.image_folder[idx]\n",
    "        image = np.array(image)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The `load_data` function prepares the dataset by applying the defined augmentations. We use the `DataLoader` from PyTorch to load the images from directories and apply preprocessing on-the-fly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_data(train_dir, val_dir, batch_size):\n",
    "    augmentations = get_augmentations()\n",
    "\n",
    "    train_dataset = AlbumentationsDataset(train_dir, transform=augmentations)\n",
    "    val_dataset = AlbumentationsDataset(val_dir, transform=augmentations)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "We define the Vision Transformer model using PyTorch. The model includes a pre-trained ViT with a custom classification head. The original classification head is removed and replaced with a fully connected layer tailored for our specific number of classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Vision Transformer model\n",
    "class ViTModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ViTModel, self).__init__()\n",
    "        self.base_model = models.vit_b_16(pretrained=True)\n",
    "        self.base_model.heads = nn.Identity()  # Remove the original classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = x[:, 0]  # Extract the CLS token\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "The `train_model` function sets up the data loaders, compiles the Vision Transformer model, and trains it using the training data. The model is evaluated on the validation set after each epoch. The best model is saved as `best_vit_model.pth`.\n",
    "\n",
    "**Note**: Update the paths for `train_dir` and `val_dir` with the actual locations of your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to train the model\n",
    "def train_model(train_loader, val_loader, num_classes, epochs=10, learning_rate=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ViTModel(num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1, min_lr=1e-6)\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Save the best model\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_vit_model.pth')\n",
    "\n",
    "    print(f'Training complete. Best validation accuracy: {best_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
